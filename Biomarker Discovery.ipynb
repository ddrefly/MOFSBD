{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import shap\n",
    "import warnings\n",
    "import json\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseline(nn.Module):\n",
    "    def __init__(self, num):\n",
    "        super(baseline, self).__init__()\n",
    "        if(num <= 32):\n",
    "            self.network = nn.Sequential(\n",
    "                nn.Linear(num, 16),\n",
    "                nn.BatchNorm1d(16),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 8),\n",
    "                nn.BatchNorm1d(8),\n",
    "                #             nn.Dropout(p=0.1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            self.network = nn.Sequential(\n",
    "                nn.Linear(num, 32),\n",
    "                nn.BatchNorm1d(32),\n",
    "                nn.Dropout(p = 0.4),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.BatchNorm1d(16),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 8),\n",
    "                nn.BatchNorm1d(8),\n",
    "                #             nn.Dropout(p=0.1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Sample data'\n",
    "\n",
    "path = './' + name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path + '/Sample data.csv')\n",
    "# data.info()\n",
    "# data['index'].value_counts()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "with open(path + '/feature.json', 'r') as f:\n",
    "    dic = json.load(f)\n",
    "dx_index = [int(x) for x in dic['dx']]\n",
    "zl_index = [int(x) for x in dic['zl']]\n",
    "\n",
    "feature = data.columns[1:]\n",
    "data[feature] = (data[feature] - data[feature].mean()) / data[feature].std()\n",
    "\n",
    "\n",
    "dx = data.iloc[:, 38:]\n",
    "zl = data.iloc[:, 1:38]\n",
    "\n",
    "dx= dx.iloc[:, dx_index]\n",
    "zl = zl.iloc[:, zl_index]\n",
    "\n",
    "ndata = pd.concat([zl, dx, data.iloc[:, 0]], axis=1)\n",
    "\n",
    "feature_size = len(dx_index) + len(zl_index)\n",
    "x_train, x_test, y_train, y_test = train_test_split(ndata.iloc[:,:feature_size], ndata.iloc[:,feature_size], test_size = 0.2, random_state=2)     #构造训练集和测试集\n",
    "\n",
    "x_train_ts = torch.tensor(x_train.values).to(torch.float32)\n",
    "y_train_ts = torch.tensor(y_train.values).to(torch.float32)\n",
    "y_train_ts = y_train_ts.reshape(y_train_ts.size()[0], 1)\n",
    "\n",
    "x_test_ts = torch.tensor(x_test.values).to(torch.float32)\n",
    "y_test_ts = torch.tensor(y_test.values).to(torch.float32)\n",
    "y_test_ts = y_test_ts.reshape(y_test_ts.size()[0], 1)\n",
    "\n",
    "tmp = torch.cat([x_train_ts, y_train_ts], dim = 1)\n",
    "tmp.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = path + '/neural_network.pth'\n",
    "\n",
    "def getScore():\n",
    "    model.eval()\n",
    "    rs = model(x_test_ts).detach().numpy().reshape(x_test_ts.size()[0])\n",
    "    pre = [0 if x < 0.05 else 1 for x in rs]\n",
    "    acc = accuracy_score(y_test_ts, pre)\n",
    "    model.train()\n",
    "    return acc\n",
    "\n",
    "epochs = 1000\n",
    "lr = 1e-2\n",
    "model = baseline(feature_size)\n",
    "model.train()\n",
    "criteon = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "loss_ls = []\n",
    "label = []\n",
    "num = 1\n",
    "ans = 0\n",
    "\n",
    "flag = True\n",
    "flag1 = True\n",
    "for epoch in range(epochs):\n",
    "    target = y_train\n",
    "    target_hat = model(x_train_ts)\n",
    "    loss = criteon(target_hat, y_train_ts)\n",
    "\n",
    "    loss_ls.append(loss.item())\n",
    "    label.append(num)\n",
    "    num += 1\n",
    "#     if(loss.item() <= 0.01):\n",
    "#         break;\n",
    "\n",
    "    acc = getScore()\n",
    "    if(acc > ans):\n",
    "        ans = acc\n",
    "        torch.save(model.state_dict(),network_path)\n",
    "        print('loss{}, acc: {}'.format(loss.item(), ans))\n",
    "        \n",
    "\n",
    "    #backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "        print(epoch, 'loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(label, loss_ls)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(network_path))\n",
    "\n",
    "model.eval()\n",
    "rs = model(x_test_ts).detach().numpy().reshape(x_test_ts.size()[0])\n",
    "pre = [0 if x < 0.05 else 1 for x in rs]\n",
    "print(\"acc：{}\".format(accuracy_score(y_test_ts, pre)))\n",
    "print(classification_report(y_test_ts, pre))\n",
    "\n",
    "rs_train = model(x_train_ts).detach().numpy().reshape(tmp.size()[0])\n",
    "pre_train = [0 if x < 0.05 else 1 for x in rs_train]\n",
    "print(\"acc：{}\".format(accuracy_score(y_train_ts, pre_train)))\n",
    "print(classification_report(y_train, pre_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_path = path + '/ROC.jpg'\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_ts, rs)\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(fpr, tpr, label='Neural Network (area = %0.2f)' % metrics.auc(fpr,tpr))\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-specificity')\n",
    "plt.ylabel('sensitivity')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.savefig(ROC_path,dpi=300,bbox_inches='tight')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : model(torch.tensor(x).to(torch.float32)).detach().numpy()\n",
    "\n",
    "explainer = shap.KernelExplainer(f, x_train.values)\n",
    "\n",
    "data_for_prediction = x_test.iloc[3,:]\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =  path + '/deep_shap.csv'\n",
    "# tmp = pd.DataFrame(shap_values[0], columns=x_train.columns, index=x_train.index)\n",
    "# tmp.to_csv(file_path)\n",
    "shap_values = pd.read_csv(file_path, index_col=0)\n",
    "shap_values = shap_values.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(shap_values, columns=x_train.columns, index=x_train.index)\n",
    "tmp = tmp.apply(lambda x : abs(x))\n",
    "tmp = tmp.apply(lambda x : x.mean()).sort_values(ascending = False)\n",
    "nn_feature = tmp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_values = explainer.shap_values(x_train)\n",
    "tmp = shap.summary_plot(shap_values, x_train)\n",
    "# plt.savefig(path + \"./deep_shap.jpg\",bbox_inches='tight') \n",
    "tmp.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 0)\n",
    "# model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "pre = model.predict(x_test)\n",
    "print(\"acc：{}\".format(accuracy_score(y_test, pre)))\n",
    "print(classification_report(y_test, pre))\n",
    "\n",
    "pre_train = model.predict(x_train)\n",
    "print(\"acc：{}\".format(accuracy_score(y_train, pre_train)))\n",
    "print(classification_report(y_train, pre_train))\n",
    "\n",
    "#ROC曲线\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pre)\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(fpr, tpr, label='Random Forest (area = %0.2f)' % metrics.auc(fpr,tpr))\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-specificity')\n",
    "plt.ylabel('sensitivity')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(path + '/ROC.jpg',dpi=300,bbox_inches='tight')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict_proba, x_train)\n",
    "\n",
    "data_for_prediction = x_train.iloc[6,:]\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path + '/machine_shap.csv'\n",
    "# tmp = pd.DataFrame(shap_values[1], columns=x_train.columns, index=x_train.index)\n",
    "# tmp.to_csv(file_path)\n",
    "shap_values = pd.read_csv(file_path, index_col=0)\n",
    "shap_values = shap_values.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_values = explainer.shap_values(x_train)\n",
    "tmp = shap.summary_plot(shap_values, x_train)\n",
    "# plt.savefig(path + '/RF_shap.jpg',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(shap_values, columns=x_train.columns, index=x_train.index)\n",
    "tmp = tmp.apply(lambda x : abs(x))\n",
    "tmp = tmp.apply(lambda x : x.mean()).sort_values(ascending = False)\n",
    "rf_feature = tmp.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biomarker Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_feature = nn_feature[:20]\n",
    "rf_feature = rf_feature[:20]\n",
    "\n",
    "marker = nn_feature.intersection(rf_feature)\n",
    "print('get {} biomarkers'.format(len(marker)))\n",
    "print(marker)\n",
    "\n",
    "col_importance = list(marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = pd.DataFrame(shap_values, columns=x_train.columns, index=x_train.index)\n",
    "sv = sv[col_importance]\n",
    "\n",
    "def func(tmp):\n",
    "    f_med = tmp.quantile(0.25)\n",
    "    med = tmp.quantile(0.5)\n",
    "    l_med = tmp.quantile(0.75)\n",
    "    iqr = l_med - f_med\n",
    "    up = l_med + 1.5 * iqr\n",
    "    low = f_med - 1.5 * iqr\n",
    "\n",
    "    sc = tmp.apply(lambda x : -2 if x < low else(\n",
    "                                            -1 if x < f_med else(\n",
    "                                            0 if x < l_med else(\n",
    "                                            1 if x < up else 2))))\n",
    "    return sc\n",
    "    \n",
    "\n",
    "score = sv.apply(func, axis = 0)\n",
    "score.columns = [x + '_sc' for x in score.columns]\n",
    "score.head()\n",
    "#getAllscore\n",
    "score['allscore'] = score.apply(lambda x : x.sum(), axis = 1)\n",
    "score['target'] = y_train\n",
    "\n",
    "score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = []\n",
    "for col in col_importance:\n",
    "    s_col = col + '_sc'\n",
    "    tmp1 = x_train[col]\n",
    "    tmp2 = score[s_col]\n",
    "    coef.append(np.corrcoef(tmp1, tmp2)[0][1])\n",
    "\n",
    "dic = {'col' : col_importance, 'coef' : coef}\n",
    "coef = pd.DataFrame(dic)\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_marker = list(coef[(coef['coef'] >= 0.7) | (coef['coef'] <= -0.7)]['col'])\n",
    "bio_marker_sc = [x + '_sc' for x in bio_marker]\n",
    "score1 = score[bio_marker_sc]\n",
    "score1['allscore'] = score.apply(lambda x : x.sum(), axis = 1)\n",
    "score1['target'] = y_train\n",
    "score1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = score1['allscore']\n",
    "y = score1['target']\n",
    "fpr, tpr, thresholds = roc_curve(y, X)\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(fpr, tpr, label='AUROC = %0.2f' % metrics.auc(fpr,tpr))\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-specificity')\n",
    "plt.ylabel('sensitivity')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC.jpg',dpi=300,bbox_inches='tight')\n",
    "plt.show();\n",
    "\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "# print('best cutoff:', optimal_threshold)\n",
    "\n",
    "y_pred = (X >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "OR = (tp * tn) / (fp * fn)\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path + '/Sample data.csv')\n",
    "feature = data.columns[0:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[bio_marker], data.iloc[:,0], test_size = 0.2, random_state=2)     #构造训练集和测试集\n",
    "\n",
    "# x_train = data[col_importance]\n",
    "# x_train = x_test\n",
    "d1 = pd.concat([x_train, score1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qjsc = pd.DataFrame(columns=['name','score','min','max'])\n",
    "pre = -1\n",
    "\n",
    "pre_name = -1\n",
    "def func(tmp):\n",
    "#     print(tmp.iloc[0,1])\n",
    "    global pre, pre_name\n",
    "    name = tmp.columns[0]\n",
    "    if(pre_name != name):\n",
    "        pre_name = name\n",
    "        low = 0\n",
    "    else:\n",
    "        low = pre\n",
    "    tmp = tmp.iloc[:, 0]\n",
    "    f_med = tmp.quantile(0.25)\n",
    "    med = tmp.quantile(0.5)\n",
    "    l_med = tmp.quantile(0.8)\n",
    "    iqr = l_med - f_med\n",
    "    up = l_med + 1.5 * iqr\n",
    "    \n",
    "    pre = l_med\n",
    "    \n",
    "    \n",
    "    t = pd.DataFrame(columns=['min','max'])\n",
    "        \n",
    "    t['min'] = [low]\n",
    "    t['max'] = [l_med]\n",
    "        \n",
    "    return t\n",
    "\n",
    "for name in bio_marker:\n",
    "    sc_name = name + '_sc'\n",
    "    if(coef[coef['col'] == name]['coef'].values[0] > 0):\n",
    "        tmp = d1[[name, sc_name]].groupby(sc_name)\n",
    "        rt = tmp.apply(func)\n",
    "    else:\n",
    "        tmp = d1[[name, sc_name]].sort_values(sc_name, ascending = False)\n",
    "        tmp = tmp.groupby(sc_name, sort = False)\n",
    "        rt = tmp.apply(func)\n",
    "    rt.index = tmp.min().index\n",
    "    rt.reset_index(inplace = True)\n",
    "    rt['name'] = sc_name\n",
    "    rt.rename({sc_name:'score'}, inplace = True, axis = 1)\n",
    "    rt = rt[['name','score','min','max']]\n",
    "    qjsc = qjsc.append(rt)\n",
    "#     print(d1[[name, sc_name]].groupby(sc_name)[name].apply(lambda x : x.describe(percentiles = [.2,.25,.5,.75,.8])))\n",
    "\n",
    "\n",
    "# qjsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(src, sc):\n",
    "#     print(sc)\n",
    "#     print(src)\n",
    "    for index in sc['score'].unique():\n",
    "        tmp = sc[sc['score'] == index]\n",
    "#         print(tmp.iloc[0, 2])\n",
    "        if(src >= tmp.iloc[0, 2] and src < tmp.iloc[0, 3]):\n",
    "            return index\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def func(tmp):\n",
    "    name = tmp.name + '_sc'\n",
    "    sc = qjsc[qjsc['name'] == name]\n",
    "    sc_tmp = tmp.apply(f1, sc = sc)\n",
    "    return sc_tmp\n",
    "    \n",
    "tmp = x_train.apply(func)\n",
    "\n",
    "# tmp.replace(-1,0, inplace = True)\n",
    "# tmp.replace(-2, 0, inplace = True)\n",
    "tmp['allscore'] = tmp.apply(lambda x: x.sum(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tmp['allscore']\n",
    "y = y_train\n",
    "# y = y_test\n",
    "# y = data.iloc[:, 0]\n",
    "fpr, tpr, thresholds = roc_curve(y, X)\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(fpr, tpr, label='AUROC = %0.2f' % metrics.auc(fpr,tpr))\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-specificity')\n",
    "plt.ylabel('sensitivity')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC.jpg',dpi=300,bbox_inches='tight')\n",
    "plt.show();\n",
    "\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "# print('best cutoff:', optimal_threshold)\n",
    "\n",
    "y_pred = (X >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "OR = (tp * tn) / (fp * fn)\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
